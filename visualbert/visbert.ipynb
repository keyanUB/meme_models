{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keyan/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import tqdm\n",
    "from tqdm import trange\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import PIL.Image\n",
    "import io\n",
    "from processing_image import Preprocess\n",
    "from visualizing_image import SingleImageViz\n",
    "from modeling_frcnn import GeneralizedRCNN\n",
    "from utils import Config\n",
    "import utils\n",
    "from transformers import VisualBertModel, BertTokenizerFast\n",
    "\n",
    "# for visualizing output\n",
    "def showarray(a, fmt=\"jpeg\"):\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    f = io.BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file cache\n",
      "loading weights file https://cdn.huggingface.co/unc-nlp/frcnn-vg-finetuned/pytorch_model.bin from cache at /home/keyan/.cache/torch/transformers/57f6df6abe353be2773f2700159c65615babf39ab5b48114d2b49267672ae10f.77b59256a4cf8343ae0f923246a81489fc8d82f98d082edc2d2037c977c0d9d0\n",
      "All model checkpoint weights were used when initializing GeneralizedRCNN.\n",
      "\n",
      "All the weights of GeneralizedRCNN were initialized from the model checkpoint at unc-nlp/frcnn-vg-finetuned.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GeneralizedRCNN for predictions without further training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at uclanlp/visualbert-vqa-coco-pre were not used when initializing VisualBertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing VisualBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing VisualBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# load models and model components\n",
    "frcnn_cfg = Config.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\")\n",
    "\n",
    "frcnn = GeneralizedRCNN.from_pretrained(\"unc-nlp/frcnn-vg-finetuned\", config=frcnn_cfg)\n",
    "\n",
    "image_preprocess = Preprocess(frcnn_cfg)\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "model = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use frcnn to get the visual embeddings\n",
    "def get_visual_embeddings(image):      \n",
    "    # image = str(image)  \n",
    "    images, sizes, scales_yx = image_preprocess(image)\n",
    "    output_dict = frcnn(\n",
    "        images,\n",
    "        sizes,\n",
    "        scales_yx=scales_yx,\n",
    "        padding=\"max_detections\",\n",
    "        max_detections=frcnn_cfg.max_detections,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    features = output_dict.get(\"roi_features\")\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inputs(images):\n",
    "    visual_embeds, visual_attention_mask, visual_token_type_ids = [], [], []\n",
    "    for img in images:\n",
    "        visual_embed = get_visual_embeddings(img)[0].unsqueeze(0)\n",
    "        visual_embeds.append(visual_embed)\n",
    "        visual_token_type_ids.append(torch.ones(visual_embed.shape[:-1], dtype=torch.long))\n",
    "        visual_attention_mask.append(torch.ones(visual_embed.shape[:-1], dtype=torch.float))\n",
    "    # convert the lists to tensors\n",
    "    visual_embeds = torch.cat(visual_embeds, dim=0)\n",
    "    visual_token_type_ids = torch.cat(visual_token_type_ids, dim=0)\n",
    "    visual_attention_mask = torch.cat(visual_attention_mask, dim=0)\n",
    "    # visual_embeds = get_visual_embeddings(images)[0].unsqueeze(0)\n",
    "    # visual_token_type_ids = torch.ones(visual_embeds.shape[:-1], dtype=torch.long)\n",
    "    # visual_attention_mask = torch.ones(visual_embeds.shape[:-1], dtype=torch.float)\n",
    "\n",
    "    return visual_embeds, visual_attention_mask, visual_token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset:  (6625, 10)\n",
      "testing dataset:  (1657, 10)\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "train = pd.read_csv('../memes_train.csv')\n",
    "test = pd.read_csv('../memes_test.csv')\n",
    "print('training dataset: ', train.shape)\n",
    "print('testing dataset: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the relatived path to the image\n",
    "train['image_path'] = '../' + train['image_path']\n",
    "test['image_path'] = '../' + test['image_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cuda if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classifier\n",
    "model = model.to(device)\n",
    "vs_embed_dim = 768  \n",
    "n_classes = 2        # number of classes for my dataset\n",
    "classifier = nn.Linear(vs_embed_dim, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read the data from the dataframe\n",
    "        row = self.df.iloc[idx]\n",
    "        image = row['image_path']\n",
    "        url = row['image_url']\n",
    "        # image = Image.open(image).convert('RGB')\n",
    "        # if self.transform:\n",
    "        #     image = Image.open(image).convert('RGB')\n",
    "        #     image = self.transform(image)        \n",
    "        text = str(row['image_text'])\n",
    "        # date = row['created_at']\n",
    "        label = row['label']\n",
    "\n",
    "        # prepare the embeddings to save time\n",
    "        # visual_embeds, visual_attention_mask, visual_token_type_ids = get_inputs(image)\n",
    "\n",
    "        return image, text, label, url\n",
    "\n",
    "# define transforms\n",
    "transform = {\n",
    "        'train' : transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                      transforms.RandomHorizontalFlip(), \n",
    "                                      transforms.RandomRotation(50),\n",
    "                                      transforms.ToTensor()]),\n",
    "\n",
    "        'valid' : transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor()])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset:  (1000, 10)\n",
      "validation dataset:  (200, 10)\n"
     ]
    }
   ],
   "source": [
    "# splite the training set into train and valid\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(train, test_size=0.2, random_state=42)\n",
    "# get 2000 samples for training, 500 for validation\n",
    "random_state = 21\n",
    "train_df = train_df.sample(n=2000, random_state=random_state)\n",
    "valid_df = valid_df.sample(n=200, random_state=random_state)\n",
    "\n",
    "print('training dataset: ', train_df.shape)\n",
    "print('validation dataset: ', valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "train_dataset = MemeDataset(train_df)\n",
    "valid_dataset = MemeDataset(valid_df)\n",
    "test_dataset = MemeDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataloader\n",
    "batch_size = 32\n",
    "num_workers = 1\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=20, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the fine-tune function\n",
    "from tqdm import tqdm\n",
    "def fine_tune_vsbert(model,     # the model to fine-tune\n",
    "                  classifier,  # single-layer fully-connected classifier\n",
    "                  dataloaders, # dict with 2 keys, \"train\" and \"valid\", containing train & valid dataloaders\n",
    "                  n_epochs,    # number of epochs to fine-tune\n",
    "                  lr,          # learning rate\n",
    "                  multi_label_data=False\n",
    "                  ):\n",
    "  \n",
    "  model.to(device)\n",
    "  classifier.to(device)\n",
    "\n",
    "  #  1. Define optimizers and loss function  \n",
    "  optimizer_encoder = optim.Adam(model.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "  optimizer_classifier = optim.Adam(classifier.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "  ### Different loss criterion for multi-label and single-label data\n",
    "  if multi_label_data:\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "  else: # single-label data (standard)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "  \n",
    "  train_losses, val_losses, val_accs = [], [], []\n",
    "  for e in trange(n_epochs, desc=\"Epoch\"):\n",
    "    #  2. Train on training data  \n",
    "    model.train()\n",
    "    classifier.train()\n",
    "    train_loss = 0.\n",
    "    for batch_i, (images, text, label, url) in tqdm(enumerate(train_loader)):\n",
    "      #  3. Forward pass\n",
    "      inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "      visual_embeds, visual_attention_mask, visual_token_type_ids = get_inputs(images)\n",
    "      inputs.update(\n",
    "          {\n",
    "              \"visual_embeds\": visual_embeds,\n",
    "              \"visual_token_type_ids\": visual_token_type_ids,\n",
    "              \"visual_attention_mask\": visual_attention_mask,\n",
    "          }\n",
    "      )\n",
    "      inputs = inputs.to(device)\n",
    "      # print(\"input loaded\")\n",
    "      outputs = model(**inputs)\n",
    "      logits = classifier(outputs.pooler_output) #get the predictions \n",
    "      # print(\"logits get\")\n",
    "      optimizer_encoder.zero_grad()\n",
    "      optimizer_classifier.zero_grad()\n",
    "\n",
    "      label = label.to(device)\n",
    "      if multi_label_data:\n",
    "        loss = criterion(logits.type(torch.FloatTensor), label.type(torch.FloatTensor))\n",
    "      else:\n",
    "        loss = criterion(logits.squeeze(-1).to(device), label)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer_encoder.step()\n",
    "      optimizer_classifier.step()\n",
    "      train_loss += loss.item()\n",
    "      print(f\"For batch {batch_i+1}/{len(train_loader)}.. \"\n",
    "            f\"Train loss: {loss.item():.3f}.. \")\n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    print(f\"Epoch {e+1}/{n_epochs}.. \"\n",
    "          f\"Train loss: {train_loss/len(train_loader):.3f}.. \") \n",
    "    # clean the cache\n",
    "    # torch.cuda.empty_cache()\n",
    "\n",
    "    #  3. Evaluate on valdiation data  \n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    val_loss = 0.\n",
    "    for batch_i, (val_images, val_text, val_label, val_url) in enumerate(valid_loader):\n",
    "      # image = Image.open(img)\n",
    "      val_inputs = tokenizer(val_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "      visual_embeds, visual_attention_mask, visual_token_type_ids = get_inputs(val_images)\n",
    "      val_inputs.update(\n",
    "          {\n",
    "              \"visual_embeds\": visual_embeds,\n",
    "              \"visual_token_type_ids\": visual_token_type_ids,\n",
    "              \"visual_attention_mask\": visual_attention_mask,\n",
    "          }\n",
    "      )\n",
    "      val_inputs = val_inputs.to(device)\n",
    "\n",
    "      val_label = val_label.to(device)\n",
    "      with torch.no_grad():\n",
    "        outputs = model(**val_inputs)\n",
    "        logits = classifier(outputs.pooler_output) #get the predictions\n",
    "        # loss = criterion(logits.type(torch.FloatTensor), label.type(torch.FloatTensor))\n",
    "        loss = criterion(logits.squeeze(-1).to(device), val_label)\n",
    "        val_loss += loss.item()\n",
    "    \n",
    "    #  4. Log results and save model checkpoints \n",
    "    print(\"Epoch: {}/{}   Val CE Loss: {:.5f}\".format(e+1, n_epochs, val_loss/len(valid_loader)))\n",
    "    val_losses.append(val_loss/len(valid_loader))\n",
    "    # save the best model\n",
    "    # if val_loss/len(valid_loader) < min(val_losses):\n",
    "    #   torch.save(model.state_dict(), '../models/vsbert_encoder.pt')\n",
    "    #   torch.save(classifier.state_dict(), '../models/vsbert_classifier.pt')\n",
    "    \n",
    "\n",
    "    # save the checkpoint\n",
    "    torch.save({\n",
    "            'epoch': e+1,\n",
    "            'encoder_state_dict': model.state_dict(),\n",
    "            'classifier_state_dict': classifier.state_dict(),\n",
    "            'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
    "            'optimizer_classifier_state_dict': optimizer_classifier.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, '../models/vsbert.pt')\n",
    "\n",
    "  return model, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tune the model\n",
    "model, classifier = fine_tune_vsbert(model, \n",
    "                                    classifier, \n",
    "                                    n_epochs=5, \n",
    "                                    lr=3e-6,\n",
    "                                    dataloaders={'train': train_loader, 'valid': valid_loader}\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the roc curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "model.eval()\n",
    "classifier.eval()\n",
    "test_loss = 0.\n",
    "\n",
    "for batch_i, (test_image, test_text, test_label) in enumerate(test_loader):\n",
    "    # inputs = processor(test_images, test_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    inputs = tokenizer(test_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    visual_embeds, visual_attention_mask, visual_token_type_ids = get_inputs(test_image)\n",
    "    inputs.update(\n",
    "        {\n",
    "            \"visual_embeds\": visual_embeds,\n",
    "            \"visual_token_type_ids\": visual_token_type_ids,\n",
    "            \"visual_attention_mask\": visual_attention_mask,\n",
    "        }\n",
    "    )\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    test_label = test_label.to(device)\n",
    "    with torch.no_grad():\n",
    "        # embeddings, att_weights = encoder(img) # embeddings: [batch_size, n_tokens, embedding dim]\n",
    "        outputs = model(**inputs)\n",
    "        logits = classifier(outputs.pooler_output) #get the predictions\n",
    "        # generate predictions\n",
    "        probs = torch.sigmoid(logits)\n",
    "        scores = probs[:,1]\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        # print(preds)\n",
    "        # print(test_label)\n",
    "        print('The number of correct predictions: ', (preds == test_label).sum().item(), '/', len(preds))\n",
    "        # break\n",
    "        \n",
    "    # get the accuracy\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] == 1 and test_label[i] == 1:\n",
    "            tp += 1\n",
    "        elif preds[i] == 0 and test_label[i] == 0:\n",
    "            tn += 1\n",
    "        elif preds[i] == 1 and test_label[i] == 0:\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1   \n",
    "    \n",
    "    for i in range(len(scores)):\n",
    "        y_true.append(test_label[i].item())\n",
    "        y_pred.append(scores[i].item())\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds})\n",
    "df.to_csv('../results/vsbert_roc.csv', index=False)\n",
    "df = pd.DataFrame({'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn}, index=[0])\n",
    "df.to_csv('../results/vsbert_cm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.read_csv('../results/vsbert_cm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = df_cm['tp'][0]\n",
    "fp = df_cm['fp'][0]\n",
    "tn = df_cm['tn'][0]\n",
    "fn = df_cm['fn'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the evaluation metrics in a table\n",
    "from prettytable import PrettyTable\n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Metric\", \"Value\"]\n",
    "x.add_row([\"Accuracy\", (tp+tn)/(tp+tn+fp+fn)])\n",
    "x.add_row([\"Precision\", tp/(tp+fp)])\n",
    "x.add_row([\"Recall\", tp/(tp+fn)])\n",
    "x.add_row([\"F1 Score\", 2*tp/(2*tp+fp+fn)])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_layout = pd.read_csv('../results/layoutlm_roc.csv')\n",
    "df_vilt = pd.read_csv('../results/vilt_roc.csv')\n",
    "df_vsbert = pd.read_csv('../results/vsbert_roc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the lists of fpr, tpr, thresholds\n",
    "fpr_layout = df_layout['fpr'].tolist()\n",
    "tpr_layout = df_layout['tpr'].tolist()\n",
    "thresholds_layout = df_layout['thresholds'].tolist()\n",
    "\n",
    "fpr_vilt = df_vilt['fpr'].tolist()\n",
    "tpr_vilt = df_vilt['tpr'].tolist()\n",
    "thresholds_vilt = df_vilt['thresholds'].tolist()\n",
    "\n",
    "fpr_vs = df_vsbert['fpr'].tolist()\n",
    "tpr_vs = df_vsbert['tpr'].tolist()\n",
    "thresholds_vs = df_vsbert['thresholds'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the figure\n",
    "import latex\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(['science','ieee', 'no-latex'])\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    # Use LaTeX default serif font.\n",
    "    \"font.serif\": [\"Times New Roman\"],\n",
    "    # Use specific cursive fonts.\n",
    "    \"font.cursive\": [\"Comic Neue\", \"Times New Roman\"],\n",
    "})\n",
    "\n",
    "# resize the figure\n",
    "# plt.rcParams['figure.figsize'] = [3, 2.5]\n",
    "\n",
    "# plot the roc curve\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "ax.plot([0, 1], [0, 1], color='black', linestyle='--')\n",
    "\n",
    "ax.plot(fpr_layout, tpr_layout, color='darkorange', label='LayoutLMv2 \\n(AUC = %0.2f)' % auc(fpr_layout, tpr_layout))\n",
    "\n",
    "ax.plot(fpr_vilt, tpr_vilt, color='darkblue', label='ViLT \\n(AUC = %0.2f)' % auc(fpr_vilt, tpr_vilt), linestyle='-')\n",
    "\n",
    "ax.plot(fpr_vs, tpr_vs, color='darkgreen', label='VisualBERT \\n(AUC = %0.2f)' % auc(fpr_vs, tpr_vs), linestyle='-.')\n",
    "\n",
    "# remove the margin\n",
    "ax.margins(0)\n",
    "\n",
    "# set the axis labels and legend\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "\n",
    "# replace 0.0 with 0 and 1.0 with 1\n",
    "ax.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "ax.set_xticklabels(['', '0.2', '0.4', '0.6', '0.8', '1'])\n",
    "ax.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "ax.set_yticklabels(['', '0.2', '0.4', '0.6', '0.8', '1'])\n",
    "\n",
    "# add 0 axis at the left bottom and 1 axis at the right top\n",
    "\n",
    "# make square\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# change the lengend box size\n",
    "ax.legend(loc='lower right', prop={'size': 7}, frameon=False)\n",
    "\n",
    "\n",
    "# save the figure\n",
    "plt.savefig('roc_curve.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570c48f12e9f0ef78305d530b27d025e0a38ceca6f3bc997bb3fb7a9fe990d66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
