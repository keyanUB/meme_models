{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import tqdm\n",
    "from tqdm import trange\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset:  (6625, 10)\n",
      "testing dataset:  (1657, 10)\n"
     ]
    }
   ],
   "source": [
    "# Read the data\n",
    "train = pd.read_csv('memes_train.csv')\n",
    "test = pd.read_csv('memes_test.csv')\n",
    "print('training dataset: ', train.shape)\n",
    "print('testing dataset: ', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cuda if available\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## install detectron2\n",
    "# !python -m pip install 'git+https://github.com/facebookresearch/detectron2.git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/layoutlmv2-base-uncased were not used when initializing LayoutLMv2Model: ['layoutlmv2.visual.backbone.bottom_up.res4.15.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.stem.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.4.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.18.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.22.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.15.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.1.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.0.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.16.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.6.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.10.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.11.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res2.1.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.2.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.1.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.7.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.8.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.2.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.17.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.5.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.3.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res5.0.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.20.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.19.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.12.conv2.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.21.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.14.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.9.conv1.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res4.13.conv3.norm.num_batches_tracked', 'layoutlmv2.visual.backbone.bottom_up.res3.3.conv2.norm.num_batches_tracked']\n",
      "- This IS expected if you are initializing LayoutLMv2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LayoutLMv2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import LayoutLMv2Processor, LayoutLMv2Model, set_seed\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "processor = LayoutLMv2Processor.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "model = LayoutLMv2Model.from_pretrained(\"microsoft/layoutlmv2-base-uncased\")\n",
    "\n",
    "image_path = \"images/Cnmi68uWEAAmxD6.jpg\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "text = \"When is coffee break?\"\n",
    "\n",
    "encoding = processor(image, text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model(**encoding)\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the classifier\n",
    "model = model.to(device)\n",
    "encoder = nn.Sequential(*[model.embeddings, model.encoder])\n",
    "LayoutLM_embed_dim = 768  # final embedding dimension for ViT\n",
    "n_classes = 2        # number of classes for my dataset\n",
    "classifier = nn.Linear(LayoutLM_embed_dim, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "class MemeDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read the data from the dataframe\n",
    "        row = self.df.iloc[idx]\n",
    "        image = row['image_path']\n",
    "        # image = Image.open(image).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = Image.open(image).convert('RGB')\n",
    "            image = self.transform(image)        \n",
    "        text = str(row['image_text'])\n",
    "        # date = row['created_at']\n",
    "        label = row['label']\n",
    "\n",
    "        return image, text, label\n",
    "\n",
    "# define transforms\n",
    "transform = {\n",
    "        'train' : transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                      transforms.RandomHorizontalFlip(), \n",
    "                                      transforms.RandomRotation(50),\n",
    "                                      transforms.ToTensor()]),\n",
    "\n",
    "        'valid' : transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor()])\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset:  (500, 10)\n",
      "validation dataset:  (100, 10)\n"
     ]
    }
   ],
   "source": [
    "# splite the training set into train and valid\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, valid_df = train_test_split(train, test_size=0.2, random_state=42)\n",
    "# get 2000 samples for training, 500 for validation\n",
    "random_state = 21\n",
    "train_df = train_df.sample(n=500, random_state=random_state)\n",
    "valid_df = valid_df.sample(n=100, random_state=random_state)\n",
    "\n",
    "print('training dataset: ', train_df.shape)\n",
    "print('validation dataset: ', valid_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "train_dataset = MemeDataset(train_df)\n",
    "valid_dataset = MemeDataset(valid_df)\n",
    "test_dataset = MemeDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataloader\n",
    "batch_size = 10\n",
    "num_workers = 2\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the fine-tune function\n",
    "def fine_tune_LayoutLM(encoder,     # ViT encoder (pre-trained)\n",
    "                  classifier,  # single-layer fully-connected classifier\n",
    "                  dataloaders, # dict with 2 keys, \"train\" and \"valid\", containing train & valid dataloaders\n",
    "                  n_epochs,    # number of epochs to fine-tune\n",
    "                  lr,          # learning rate\n",
    "                  multi_label_data=False\n",
    "                  ):\n",
    "  \n",
    "  encoder.to(device)\n",
    "  classifier.to(device)\n",
    "\n",
    "  #  1. Define optimizers and loss function  \n",
    "  optimizer_encoder = optim.Adam(encoder.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "  optimizer_classifier = optim.Adam(classifier.parameters(), lr=lr, betas=(0.5, 0.9))\n",
    "  ### Different loss criterion for multi-label and single-label data\n",
    "  if multi_label_data:\n",
    "    criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "  else: # single-label data (standard)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "  \n",
    "  train_losses, val_losses, val_accs = [], [], []\n",
    "  for e in trange(n_epochs, desc=\"Epoch\"):\n",
    "    #  2. Train on training data  \n",
    "    encoder.train()\n",
    "    classifier.train()\n",
    "    train_loss = 0.\n",
    "    for batch_i, (image, text, label) in enumerate(train_loader):\n",
    "      # image = Image.open(requests.get(img, stream=True).raw)\n",
    "      # image = Image.open(img)\n",
    "      images = []\n",
    "      for img in image:\n",
    "        images.append(Image.open(img).convert('RGB'))\n",
    "      # texts = []\n",
    "      # for txt in text:\n",
    "      #   texts.append(txt)\n",
    "      inputs = processor(images, text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "      outputs = model(**inputs)\n",
    "      logits = classifier(outputs.pooler_output) #get the predictions \n",
    "           \n",
    "      optimizer_encoder.zero_grad()\n",
    "      optimizer_classifier.zero_grad()\n",
    "\n",
    "      label = label.to(device)\n",
    "      if multi_label_data:\n",
    "        loss = criterion(logits.type(torch.FloatTensor), label.type(torch.FloatTensor))\n",
    "      else:\n",
    "        loss = criterion(logits.squeeze(-1).to(device), label)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer_encoder.step()\n",
    "      optimizer_classifier.step()\n",
    "      train_loss += loss.item()\n",
    "      # print(f\"For batch {batch_i+1}/{len(train_loader)}.. \"\n",
    "      #       f\"Train loss: {loss.item():.3f}.. \")\n",
    "    train_losses.append(train_loss/len(train_loader))\n",
    "    print(f\"Epoch {e+1}/{n_epochs}.. \"\n",
    "          f\"Train loss: {train_loss/len(train_loader):.3f}.. \") \n",
    "    \n",
    "\n",
    "    #  3. Evaluate on valdiation data  \n",
    "    encoder.eval()\n",
    "    classifier.eval()\n",
    "    val_loss = 0.\n",
    "    for batch_i, (val_image, val_text, val_label) in enumerate(valid_loader):\n",
    "      # image = Image.open(img)\n",
    "      val_images = []\n",
    "      for img in val_image:\n",
    "        val_images.append(Image.open(img).convert('RGB'))\n",
    "      # texts = []\n",
    "      # for txt in text:\n",
    "      #   texts.append(txt)\n",
    "      inputs = processor(val_images, val_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "      val_label = val_label.to(device)\n",
    "      with torch.no_grad():\n",
    "        # embeddings, att_weights = encoder(img) # embeddings: [batch_size, n_tokens, embedding dim]\n",
    "        outputs = model(**inputs)\n",
    "        logits = classifier(outputs.pooler_output) #get the predictions\n",
    "        # loss = criterion(logits.type(torch.FloatTensor), label.type(torch.FloatTensor))\n",
    "        loss = criterion(logits.squeeze(-1).to(device), val_label)\n",
    "        val_loss += loss.item()\n",
    "    \n",
    "    #  4. Log results and save model checkpoints \n",
    "    print(\"Epoch: {}/{}   Val CE Loss: {:.5f}\".format(e+1, n_epochs, val_loss/len(valid_loader)))\n",
    "    \n",
    "    # torch.save(encoder.state_dict(), 'encoder_{}.pt'.format(e+1))\n",
    "    # torch.save(classifier.state_dict(), 'classifier_{}.pt'.format(e+1))\n",
    "    val_losses.append(val_loss/len(valid_loader))\n",
    "    # # save the best model\n",
    "    if val_loss/len(valid_loader) < min(val_losses):\n",
    "      torch.save(encoder.state_dict(), 'models/layoutlm_best_encoder.pt')\n",
    "      torch.save(classifier.state_dict(), 'models/layoutlm_best_classifier.pt')\n",
    "    \n",
    "\n",
    "    # save the checkpoint\n",
    "    torch.save({\n",
    "            'epoch': e+1,\n",
    "            'encoder_state_dict': encoder.state_dict(),\n",
    "            'classifier_state_dict': classifier.state_dict(),\n",
    "            'optimizer_encoder_state_dict': optimizer_encoder.state_dict(),\n",
    "            'optimizer_classifier_state_dict': optimizer_classifier.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, 'models/layoutlm.pt')\n",
    "\n",
    "  return encoder, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the checkpoint\n",
    "# checkpoint = torch.load('models/layoutlm.pt')\n",
    "# encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "# classifier.load_state_dict(checkpoint['classifier_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# fine-tune the model\n",
    "encoder, classifier = fine_tune_LayoutLM(encoder, \n",
    "                                    classifier, \n",
    "                                    n_epochs=5, \n",
    "                                    lr=1e-4,\n",
    "                                    dataloaders={'train': train_loader, 'valid': valid_loader}\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean cuda cache\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the best model\n",
    "encoder.load_state_dict(torch.load('model/layoutlm_best_encoder.pt'))\n",
    "classifier.load_state_dict(torch.load('model/layoutlm_best_classifier.pt'))\n",
    "classifier = classifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the roc curve\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tp, fp, tn, fn = 0, 0, 0, 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "encoder.eval()\n",
    "classifier.eval()\n",
    "test_loss = 0.\n",
    "\n",
    "for batch_i, (test_image, test_text, test_label) in enumerate(test_loader):\n",
    "    # image = Image.open(img)\n",
    "    test_images = []\n",
    "    for img in test_image:\n",
    "        test_images.append(Image.open(img).convert('RGB'))\n",
    "    inputs = processor(test_images, test_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    test_label = test_label.to(device)\n",
    "    with torch.no_grad():\n",
    "        # embeddings, att_weights = encoder(img) # embeddings: [batch_size, n_tokens, embedding dim]\n",
    "        outputs = model(**inputs)\n",
    "        logits = classifier(outputs.pooler_output) #get the predictions\n",
    "        # generate predictions\n",
    "        probs = torch.sigmoid(logits)\n",
    "        scores = probs[:,1]\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        # print(preds)\n",
    "        # print(test_label)\n",
    "        print('The number of correct predictions: ', (preds == test_label).sum().item(), '/', len(preds))\n",
    "        # break\n",
    "        \n",
    "    # get the accuracy\n",
    "    for i in range(len(preds)):\n",
    "        if preds[i] == 1 and test_label[i] == 1:\n",
    "            tp += 1\n",
    "        elif preds[i] == 0 and test_label[i] == 0:\n",
    "            tn += 1\n",
    "        elif preds[i] == 1 and test_label[i] == 0:\n",
    "            fp += 1\n",
    "        else:\n",
    "            fn += 1   \n",
    "    \n",
    "    for i in range(len(scores)):\n",
    "        y_true.append(test_label[i].item())\n",
    "        y_pred.append(scores[i].item())\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds})\n",
    "df.to_csv('results/vit_roc.csv', index=False)\n",
    "df = pd.DataFrame({'tp': tp, 'fp': fp, 'tn': tn, 'fn': fn}, index=[0])\n",
    "df.to_csv('results/vit_cm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.read_csv('results/layoutlm_cm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = df_cm['tp'][0]\n",
    "fp = df_cm['fp'][0]\n",
    "tn = df_cm['tn'][0]\n",
    "fn = df_cm['fn'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the evaluation metrics in a table\n",
    "from prettytable import PrettyTable\n",
    "x = PrettyTable()\n",
    "x.field_names = [\"Metric\", \"Value\"]\n",
    "x.add_row([\"Accuracy\", (tp+tn)/(tp+tn+fp+fn)])\n",
    "x.add_row([\"Precision\", tp/(tp+fp)])\n",
    "x.add_row([\"Recall\", tp/(tp+fn)])\n",
    "x.add_row([\"F1 Score\", 2*tp/(2*tp+fp+fn)])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.read_csv('results/layoutlm_roc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the lists of fpr, tpr, thresholds\n",
    "fpr = df_result['fpr'].tolist()\n",
    "tpr = df_result['tpr'].tolist()\n",
    "thresholds_list = df_result['thresholds'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the roc curve\n",
    "import latex\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "\n",
    "plt.style.use(['science','ieee', 'no-latex'])\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"serif\",\n",
    "    # Use LaTeX default serif font.\n",
    "    \"font.serif\": [\"Times New Roman\"],\n",
    "    # Use specific cursive fonts.\n",
    "    \"font.cursive\": [\"Comic Neue\", \"Times New Roman\"],\n",
    "})\n",
    "# plt.rcParams['font.family'] = 'serif'\n",
    "# plt.rcParams['font.serif'] = ['Times New Roman'] + plt.rcParams['font.serif'] \n",
    "# # lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange', label='ViT (area = %0.2f)' % auc(fpr, tpr))\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc=\"lower right\")\n",
    "# plt.title('Receiver operating characteristic example')\n",
    "# plt.show()\n",
    "plt.savefig('roc_curve.pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "30104676fc1f11152a662b3d1ac6a86278087a9fbbe46acdab4072763e5a483f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
